{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49421721-19e4-492d-8aee-fe1119e3af13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENTITY EXTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd6fe8-826a-462a-a62b-8945b8c9b312",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PLACE OF ACCIDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75988c-c090-4d12-96c3-a9f26a2fba73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "places = df['content'].str.split(':').str[0].str.strip()\n",
    "df['place'] = places\n",
    "df['place'] = df['place'].str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d329a-5ac5-4d15-b1a4-457a683e3527",
   "metadata": {
    "id": "8vGcLC2RDJ-c",
    "tags": []
   },
   "source": [
    "## CREATING IDs FOR EACH NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015f5e56-b5ca-4823-ab0a-422d64aaa667",
   "metadata": {
    "id": "-cPZfnGHDJ-c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract id from link\n",
    "def extract_id(link):\n",
    "    match = re.search(r'\\b(\\d{6,11})\\b', link)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to extract ids\n",
    "df['id'] = df['Link'].apply(extract_id)\n",
    "order = ['id','place','Link','content','News_date','First_Line']\n",
    "df = df[order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee721a-561d-4f5c-8619-3acb1254bf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split 'place' column by spaces and calculate token count\n",
    "df['places_token_count'] = df['place'].str.split(' ').apply(lambda x: len(x) if isinstance(x, list) else np.nan)\n",
    "\n",
    "# Replace NaN values with 0\n",
    "df['places_token_count'] = df['places_token_count'].fillna(0)\n",
    "\n",
    "# Convert to integer\n",
    "df['places_token_count'] = df['places_token_count'].astype(int)\n",
    "\n",
    "# Display the updated dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f4771-2887-4622-bd12-fa4ca447870c",
   "metadata": {},
   "source": [
    "### Verifying if places_token_count values above 5 are unwanted news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6257e33-35df-46cc-8be3-1c14de13e5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counts_1 = len(df[df['places_token_count'] == 1])\n",
    "counts_2 = len(df[df['places_token_count'] == 2])\n",
    "counts_3 = len(df[df['places_token_count'] == 3])\n",
    "counts_4 = len(df[df['places_token_count'] == 4])\n",
    "counts_5 = len(df[df['places_token_count'] == 5])\n",
    "counts_above_5 = len(df[df['places_token_count'] > 5])\n",
    "categories = ['1', '2', '3', '4', '5', 'Above 5']\n",
    "counts = [counts_1, counts_2, counts_3, counts_4, counts_5, counts_above_5]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, edgecolor='black')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Token Counts in places_token_count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757dba0-7216-4b5f-8c71-4850b1453013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(columns =['places_token_count'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e31c79-5bff-45e9-8316-569239598ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('dataframe_clustering.csv', index =False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05a5385-873c-42f4-8631-317d1909fafd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LATITUDE AND LONGITUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddcee2-f10a-48f1-8afa-5a87b4e2bc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"geocode_api.txt\") as apikey_file:\n",
    "    apikey = apikey_file.readline()\n",
    "# Function to get latitude and longitude for a place\n",
    "def get_lat_long(place):\n",
    "    if len(place) > 30:\n",
    "        print(f\"Skipping place '{place}' as length is more than 30 characters.\")\n",
    "        return None, None\n",
    "    \n",
    "    url = f\"https://geocode.maps.co/search?q={place.replace(' ', '+')}&api_key={apikey}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                return data[0]['lat'], data[0]['lon']\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Apply function to DataFrame with delay\n",
    "df['latitude'] = None\n",
    "df['longitude'] = None\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if len(row['place']) <= 30:\n",
    "        latitude, longitude = get_lat_long(row['place'])\n",
    "        df.at[index, 'latitude'] = latitude\n",
    "        df.at[index, 'longitude'] = longitude\n",
    "    else:\n",
    "        print(f\"Skipping place '{row['place']}' as length is more than 30 characters.\")\n",
    "    time.sleep(1)  # Delay of 1 second\n",
    "\n",
    "# Display the DataFrame with latitude and longitude\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764d420-f7b6-43f9-8072-6f07721c90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataframe_lat_long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a8233-b3ba-4195-8968-7e95f3c1f063",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e9f82d-9caf-42f6-b6bb-02d72e7ea231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "states_data = pd.read_csv('dataframe_lat_long.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46de7f78-95c1-4b48-af4c-f4fca55551ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "place = list(states_data['place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f6c51fc-9be7-4e59-adf2-65cce3b6735c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import get_geocoder_for_service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4048acb9-085c-4b2d-b1e7-eab20aee9a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def geocode(geocoder, config, query):\n",
    "    cls = get_geocoder_for_service(geocoder)\n",
    "    geolocator = cls(**config)\n",
    "    location = geolocator.geocode(query)\n",
    "    return location.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76490c78-7726-4133-a43f-4524d1157889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 6750/6750 [40:02<00:00,  2.81it/s]   \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Dictionary of Indian states\n",
    "indian_states = {\n",
    "    'Andaman and Nicobar Islands', 'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chandigarh', 'Chhattisgarh',\n",
    "    'Dadra and Nagar Haveli and Daman and Diu', 'Delhi', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jammu and Kashmir',\n",
    "    'Jharkhand', 'Karnataka', 'Kerala', 'Ladakh', 'Lakshadweep', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya',\n",
    "    'Mizoram', 'Nagaland', 'Odisha', 'Puducherry', 'Punjab', 'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura',\n",
    "    'Uttar Pradesh', 'Uttarakhand', 'West Bengal'\n",
    "}\n",
    "\n",
    "states = []\n",
    "\n",
    "for i in tqdm(range(len(place)), desc=\"Processing\"):\n",
    "    try:\n",
    "        split_result = geocode(\"nominatim\", dict(user_agent=\"aakarshsurendra\"), place[i]).split(',')\n",
    "        state = None\n",
    "        for part in split_result:\n",
    "            part = part.strip()\n",
    "            if part in indian_states:\n",
    "                state = part\n",
    "                break\n",
    "            elif part and not any(char.isdigit() or char.isalpha() for char in part):\n",
    "                # Skip parts that are not alphanumeric (e.g., blank, unwanted characters)\n",
    "                continue\n",
    "        states.append(state)\n",
    "    except:\n",
    "        states.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "485130ee-48f4-4caf-8504-5a511b30f9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states_data['state']=states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ff83f58-73b4-4ace-bb59-5f24d49147bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "states_data.to_csv('dataframe_states.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04bd9a-096c-452c-810b-7cd132b77f69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## WEATHER AND PRECIPITATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691f82e-ee71-48af-b604-f36f342725f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openmeteo-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90647540-c6ea-497a-bfec-9ac8c96295f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03e754c1-92d1-49d9-856f-b11140c4c046",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating weather: 100%|██████████| 6750/6750 [01:36<00:00, 69.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              place   News_date  week_avg_weather\n",
      "0                         Sultanpur  2024-02-23         18.672459\n",
      "1                            Jaipur  2024-02-19         18.577219\n",
      "2                           Raichur  2024-02-18         29.642168\n",
      "3                         New Delhi  2024-02-16         18.050730\n",
      "4                         Hyderabad  2024-02-22         26.445761\n",
      "...                             ...         ...               ...\n",
      "6745                         Rajkot  2019-07-30         27.055136\n",
      "6746                      Bengaluru  2019-05-28         25.609011\n",
      "6747                Ambala/Parwanoo  2019-05-04               NaN\n",
      "6748                        Madurai  2019-06-04         30.240919\n",
      "6749  Padiyan Ka Purwa (Rae Bareli)  2018-05-10               NaN\n",
      "\n",
      "[6750 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('dataframe_states.csv')\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def extract_date(date_str):\n",
    "    date_str = date_str.strip()  # Remove leading and trailing whitespaces\n",
    "    try:\n",
    "        return datetime.strptime(date_str, 'Updated: %b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return datetime.strptime(date_str, '%b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "\n",
    "def get_week_avg_weather(lat, lon, start_date, end_date):\n",
    "    if pd.isnull(lat):  # Skip rows with missing latitude\n",
    "        return None\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "    responses = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=params)\n",
    "    response = responses[0]  # Assuming only one location is being queried\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    return pd.Series(hourly_temperature_2m).mean()\n",
    "\n",
    "# Add a new column 'week_avg_weather' to the DataFrame\n",
    "df['News_date'] = df['News_date'].apply(extract_date)\n",
    "\n",
    "tqdm.pandas(desc=\"Calculating weather\")\n",
    "df['week_avg_weather'] = df.progress_apply(lambda row: get_week_avg_weather(row['latitude'], row['longitude'], row['News_date'], (datetime.strptime(row['News_date'], '%Y-%m-%d') + timedelta(days=7)).strftime('%Y-%m-%d')), axis=1)\n",
    "\n",
    "print(df[['place', 'News_date', 'week_avg_weather']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffcb37d-e84d-4aa4-a5eb-f184a654a7a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('dataset_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98510a12-6ff0-4fa8-afa0-5ed49760365e",
   "metadata": {},
   "source": [
    "### Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08846100-f8a2-452c-ba8f-aab52c8d79c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [07:56<00:00, 14.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              place   News_date  precipitation_3days\n",
      "0                         Sultanpur  2024-02-23             0.200000\n",
      "1                            Jaipur  2024-02-19             0.000000\n",
      "2                           Raichur  2024-02-18             0.000000\n",
      "3                         New Delhi  2024-02-16             0.000000\n",
      "4                         Hyderabad  2024-02-22             0.000000\n",
      "...                             ...         ...                  ...\n",
      "6745                         Rajkot  2019-07-30            73.299995\n",
      "6746                      Bengaluru  2019-05-28             5.900000\n",
      "6747                Ambala/Parwanoo  2019-05-04                  NaN\n",
      "6748                        Madurai  2019-06-04            30.000000\n",
      "6749  Padiyan Ka Purwa (Rae Bareli)  2018-05-10                  NaN\n",
      "\n",
      "[6750 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('dataset_weather.csv')\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "def extract_date(date_str):\n",
    "    date_str = date_str.strip()  # Remove leading and trailing whitespaces\n",
    "    try:\n",
    "        return datetime.strptime(date_str, 'Updated: %b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return datetime.strptime(date_str, '%b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "\n",
    "def get_precipitation_3days(lat, lon, start_date):\n",
    "    if pd.isnull(lat):  # Skip rows with missing latitude\n",
    "        return None\n",
    "    \n",
    "    total_precipitation = 0\n",
    "    for i in range(-1, 2):  # Loop for the current day and the two days before and after\n",
    "        date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": date,\n",
    "            \"end_date\": date,\n",
    "            \"hourly\": \"precipitation\"\n",
    "        }\n",
    "        responses = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=params)\n",
    "        response = responses[0]  # Assuming only one location is being queried\n",
    "        hourly_precipitation = response.Hourly().Variables(0).ValuesAsNumpy()\n",
    "        total_precipitation += hourly_precipitation.sum()\n",
    "    \n",
    "    return total_precipitation\n",
    "\n",
    "# Add a new column 'precipitation_3days' to the DataFrame\n",
    "df['News_date'] = df['News_date'].apply(extract_date)\n",
    "\n",
    "tqdm.pandas()\n",
    "df['precipitation_3days'] = df.progress_apply(lambda row: get_precipitation_3days(row['latitude'], row['longitude'], row['News_date']), axis=1)\n",
    "\n",
    "print(df[['place', 'News_date', 'precipitation_3days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154cd162-4704-404c-9893-966a30efa94b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the processed data to a CSV file\n",
    "df.to_csv('entity_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07c940-aab1-4d04-8880-e6da12ccc694",
   "metadata": {},
   "source": [
    "### Combining Small Cities for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d656a7df-12e9-43c3-b1fb-3d07b1add038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_csv('dataframe_states.csv')\n",
    "\n",
    "# Find major cities (places with more than 20 occurrences)\n",
    "major_cities = df['place'].value_counts()[df['place'].value_counts() > 20].index.tolist()\n",
    "\n",
    "# Initialize a dictionary to store combined city data\n",
    "combined_cities = {}\n",
    "\n",
    "# Iterate through each major city\n",
    "for city in major_cities:\n",
    "    # Find other cities within +/- 0.2 latitude and longitude difference\n",
    "    nearby_cities = df[(df['place'] != city) & \n",
    "                       (df['latitude'].between(df[df['place'] == city]['latitude'].iloc[0] - 0.2, \n",
    "                                               df[df['place'] == city]['latitude'].iloc[0] + 0.2)) &\n",
    "                       (df['longitude'].between(df[df['place'] == city]['longitude'].iloc[0] - 0.2, \n",
    "                                                df[df['place'] == city]['longitude'].iloc[0] + 0.2))]\n",
    "    \n",
    "    # Combine the cities into the major city\n",
    "    combined_cities[city] = nearby_cities['place'].tolist()\n",
    "\n",
    "# Update the DataFrame with the combined city names and adjust latitudes and longitudes\n",
    "for major_city, cities_to_combine in combined_cities.items():\n",
    "    # Update place names to the major city name\n",
    "    df.loc[df['place'].isin(cities_to_combine), 'place'] = major_city\n",
    "    # Update latitudes and longitudes to the major city's values\n",
    "    df.loc[df['place'] == major_city, 'latitude'] = df[df['place'] == major_city]['latitude'].mean()\n",
    "    df.loc[df['place'] == major_city, 'longitude'] = df[df['place'] == major_city]['longitude'].mean()\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel('updated_cities.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
