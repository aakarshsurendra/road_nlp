{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c70c7b3-11b7-46c0-81ac-e4e32dd684b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f6a09f-0866-449a-91fa-59372132c78b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b43000-f67d-4a4d-8846-30ffc51e7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openmeteo-requests\n",
    "pip install requests-cache retry-requests numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c0ed1-e80b-447a-a578-35d753b43b90",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "629eca1d-9e4f-4efa-8aed-9f4d481f6840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aakarshsurendra/Desktop/ROAD_NLP/01_extraction'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a1010d-809b-4e03-98c5-3b6f9f1c167b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/input/filtered_dataset_oneliner.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/input/filtered_dataset_oneliner.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/input/filtered_dataset_oneliner.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/input/filtered_dataset_oneliner.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70456125-348d-4e82-b0e3-9b3f9284d556",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0eae3d-ca98-4c7e-8dbb-8ba93b3b20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get Latitude and Longitudes\n",
    "def get_lat_long(place):\n",
    "    if len(place) > 30:\n",
    "        print(f\"Skipping place '{place}' as length is more than 30 characters.\")\n",
    "        return None, None\n",
    "    \n",
    "    url = f\"https://geocode.maps.co/search?q={place.replace(' ', '+')}&api_key={apikey}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            if data:\n",
    "                return data[0]['lat'], data[0]['lon']\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3f84d-eedf-43f1-a48e-9a34061f4b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract states\n",
    "def geocode(geocoder, config, query):\n",
    "    cls = get_geocoder_for_service(geocoder)\n",
    "    geolocator = cls(**config)\n",
    "    location = geolocator.geocode(query)\n",
    "    return location.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb0ef7-b790-4678-9f9e-c6bc9c1e13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting dates and Average Weather Functions\n",
    "\n",
    "def extract_date(date_str):\n",
    "    date_str = date_str.strip()  # Remove leading and trailing whitespaces\n",
    "    try:\n",
    "        return datetime.strptime(date_str, 'Updated: %b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return datetime.strptime(date_str, '%b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "\n",
    "def get_week_avg_weather(lat, lon, start_date, end_date):\n",
    "    if pd.isnull(lat):  # Skip rows with missing latitude\n",
    "        return None\n",
    "    \n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"hourly\": \"temperature_2m\"\n",
    "    }\n",
    "    responses = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=params)\n",
    "    response = responses[0]  # Assuming only one location is being queried\n",
    "    hourly = response.Hourly()\n",
    "    hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "    return pd.Series(hourly_temperature_2m).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef1bd2-8ac1-4071-9766-5a308153b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting date for precipitation\n",
    "def extract_date(date_str):\n",
    "    date_str = date_str.strip()  # Remove leading and trailing whitespaces\n",
    "    try:\n",
    "        return datetime.strptime(date_str, 'Updated: %b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return datetime.strptime(date_str, '%b %d, %Y, %H:%M IST').strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a7944-eedc-4378-a2fd-b83651579532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exracting Precipitation Data - 3 day average\n",
    "\n",
    "def get_precipitation_3days(lat, lon, start_date):\n",
    "    if pd.isnull(lat):  # Skip rows with missing latitude\n",
    "        return None\n",
    "    \n",
    "    total_precipitation = 0\n",
    "    for i in range(-1, 2):  # Loop for the current day and the two days before and after\n",
    "        date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=i)).strftime('%Y-%m-%d')\n",
    "        params = {\n",
    "            \"latitude\": lat,\n",
    "            \"longitude\": lon,\n",
    "            \"start_date\": date,\n",
    "            \"end_date\": date,\n",
    "            \"hourly\": \"precipitation\"\n",
    "        }\n",
    "        responses = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=params)\n",
    "        response = responses[0]  # Assuming only one location is being queried\n",
    "        hourly_precipitation = response.Hourly().Variables(0).ValuesAsNumpy()\n",
    "        total_precipitation += hourly_precipitation.sum()\n",
    "    \n",
    "    return total_precipitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69bf39-408e-4188-96ef-88b6f84829e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PLACE OF ACCIDENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffec903-b587-43fc-99b5-9dcb774ad1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split 'content' column by ':' and extract the first part\n",
    "df['place'] = df['content'].str.split(':').str[0].str.strip().str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc54572-9f2d-452d-a484-0b713bbb860b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# LATITUDE AND LONGITUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98176e63-a619-47c5-966a-2f51c7e68d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"geocode_api.txt\") as apikey_file:\n",
    "    apikey = apikey_file.readline()\n",
    "# Function to get latitude and longitude for a place\n",
    "\n",
    "# Apply function to DataFrame with delay\n",
    "df['latitude'] = None\n",
    "df['longitude'] = None\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    if len(row['place']) <= 30:\n",
    "        latitude, longitude = get_lat_long(row['place'])\n",
    "        df.at[index, 'latitude'] = latitude\n",
    "        df.at[index, 'longitude'] = longitude\n",
    "    else:\n",
    "        print(f\"Skipping place '{row['place']}' as length is more than 30 characters.\")\n",
    "    time.sleep(1)  # Delay of 1 second\n",
    "\n",
    "# Display the DataFrame with latitude and longitude\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e208f-f5a4-4d07-b99c-d384459f0c1b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# STATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785c31bc-fe2b-4535-83de-f3b003eb471e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "place = list(df['place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189942e7-004b-4142-9652-6736e778933a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 6750/6750 [40:02<00:00,  2.81it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Dictionary of Indian states\n",
    "indian_states = {\n",
    "    'Andaman and Nicobar Islands', 'Andhra Pradesh', 'Arunachal Pradesh', 'Assam', 'Bihar', 'Chandigarh', 'Chhattisgarh',\n",
    "    'Dadra and Nagar Haveli and Daman and Diu', 'Delhi', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', 'Jammu and Kashmir',\n",
    "    'Jharkhand', 'Karnataka', 'Kerala', 'Ladakh', 'Lakshadweep', 'Madhya Pradesh', 'Maharashtra', 'Manipur', 'Meghalaya',\n",
    "    'Mizoram', 'Nagaland', 'Odisha', 'Puducherry', 'Punjab', 'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura',\n",
    "    'Uttar Pradesh', 'Uttarakhand', 'West Bengal'\n",
    "}\n",
    "\n",
    "states = []\n",
    "\n",
    "for i in tqdm(range(len(place)), desc=\"Processing\"):\n",
    "    try:\n",
    "        split_result = geocode(\"nominatim\", dict(user_agent=\"aakarshsurendra\"), place[i]).split(',')\n",
    "        state = None\n",
    "        for part in split_result:\n",
    "            part = part.strip()\n",
    "            if part in indian_states:\n",
    "                state = part\n",
    "                break\n",
    "            elif part and not any(char.isdigit() or char.isalpha() for char in part):\n",
    "                # Skip parts that are not alphanumeric (e.g., blank, unwanted characters)\n",
    "                continue\n",
    "        states.append(state)\n",
    "    except:\n",
    "        states.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d27bcdaf-a249-4f6b-b179-329959f1a98a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['state']=states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d23153-51be-4d72-82fe-2f52104b11ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# WEATHER AND PRECIPITATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bde638-29fd-4027-9b3a-6c0b3f7682de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating weather: 100%|██████████| 6750/6750 [01:36<00:00, 69.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              place   News_date  week_avg_weather\n",
      "0                         Sultanpur  2024-02-23         18.672459\n",
      "1                            Jaipur  2024-02-19         18.577219\n",
      "2                           Raichur  2024-02-18         29.642168\n",
      "3                         New Delhi  2024-02-16         18.050730\n",
      "4                         Hyderabad  2024-02-22         26.445761\n",
      "...                             ...         ...               ...\n",
      "6745                         Rajkot  2019-07-30         27.055136\n",
      "6746                      Bengaluru  2019-05-28         25.609011\n",
      "6747                Ambala/Parwanoo  2019-05-04               NaN\n",
      "6748                        Madurai  2019-06-04         30.240919\n",
      "6749  Padiyan Ka Purwa (Rae Bareli)  2018-05-10               NaN\n",
      "\n",
      "[6750 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Add a new column 'week_avg_weather' to the DataFrame\n",
    "df['news_date'] = df['news_date'].apply(extract_date)\n",
    "\n",
    "tqdm.pandas(desc=\"Calculating weather\")\n",
    "df['week_avg_weather'] = df.progress_apply(lambda row: get_week_avg_weather(row['latitude'], row['longitude'], row['News_date'], (datetime.strptime(row['News_date'], '%Y-%m-%d') + timedelta(days=7)).strftime('%Y-%m-%d')), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade89ac-d853-47b1-9e14-75a5643e28ae",
   "metadata": {},
   "source": [
    "## Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f5251f-d11e-4243-8bbb-5b69a6becd11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6750/6750 [07:56<00:00, 14.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              place   News_date  precipitation_3days\n",
      "0                         Sultanpur  2024-02-23             0.200000\n",
      "1                            Jaipur  2024-02-19             0.000000\n",
      "2                           Raichur  2024-02-18             0.000000\n",
      "3                         New Delhi  2024-02-16             0.000000\n",
      "4                         Hyderabad  2024-02-22             0.000000\n",
      "...                             ...         ...                  ...\n",
      "6745                         Rajkot  2019-07-30            73.299995\n",
      "6746                      Bengaluru  2019-05-28             5.900000\n",
      "6747                Ambala/Parwanoo  2019-05-04                  NaN\n",
      "6748                        Madurai  2019-06-04            30.000000\n",
      "6749  Padiyan Ka Purwa (Rae Bareli)  2018-05-10                  NaN\n",
      "\n",
      "[6750 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Add a new column 'precipitation_3days' to the DataFrame\n",
    "df['News_date'] = df['News_date'].apply(extract_date)\n",
    "\n",
    "tqdm.pandas()\n",
    "df['precipitation_3days'] = df.progress_apply(lambda row: get_precipitation_3days(row['latitude'], row['longitude'], row['News_date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa007c43-a131-49bd-949c-429ed0507520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output\\entity_extracted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4c2fc-489d-424d-8495-7bc62dc9ae6d",
   "metadata": {},
   "source": [
    "### Optional - Combining Small Cities for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef863f6c-836c-4346-bed5-fcc1adc401dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Find major cities (places with more than 20 occurrences)\n",
    "major_cities = df['place'].value_counts()[df['place'].value_counts() > 20].index.tolist()\n",
    "\n",
    "# Initialize a dictionary to store combined city data\n",
    "combined_cities = {}\n",
    "\n",
    "# Iterate through each major city\n",
    "for city in major_cities:\n",
    "    # Find other cities within +/- 0.2 latitude and longitude difference\n",
    "    nearby_cities = df[(df['place'] != city) & \n",
    "                       (df['latitude'].between(df[df['place'] == city]['latitude'].iloc[0] - 0.2, \n",
    "                                               df[df['place'] == city]['latitude'].iloc[0] + 0.2)) &\n",
    "                       (df['longitude'].between(df[df['place'] == city]['longitude'].iloc[0] - 0.2, \n",
    "                                                df[df['place'] == city]['longitude'].iloc[0] + 0.2))]\n",
    "    \n",
    "    # Combine the cities into the major city\n",
    "    combined_cities[city] = nearby_cities['place'].tolist()\n",
    "\n",
    "# Update the DataFrame with the combined city names and adjust latitudes and longitudes\n",
    "for major_city, cities_to_combine in combined_cities.items():\n",
    "    # Update place names to the major city name\n",
    "    df.loc[df['place'].isin(cities_to_combine), 'place'] = major_city\n",
    "    # Update latitudes and longitudes to the major city's values\n",
    "    df.loc[df['place'] == major_city, 'latitude'] = df[df['place'] == major_city]['latitude'].mean()\n",
    "    df.loc[df['place'] == major_city, 'longitude'] = df[df['place'] == major_city]['longitude'].mean()\n",
    "\n",
    "# Save the updated DataFrame to a new Excel file\n",
    "df.to_excel('updated_cities.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
